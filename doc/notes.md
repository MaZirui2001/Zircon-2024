# Zircon2024开发心得与思考

## 2025-01-31

今天对DCache架构进行了思考。主要的成果如下：

1. True Dual BRAM实现了store提交的“超车”

2. DCache的违例情况：

    - uncache store：需要保证之前的所有读写都完成，且之后的所有读操作都不能执行；
        - 所有uncache写会停留在c1s3中，直至**c2s3中该指令的rob_idx出现**。
    - uncache load：需要保证之前的所有读写都完成
        - 保证之前的读的顺序：乱序发射出的load会被强制送回发射队列重新执行
        - 保证之前的写的顺序：所有uncache读会停留在c1s3中，直至**该指令成为ROB中最后一条指令，且Store Buffer所有待提交指令全部完成执行**。

## 2025-02-01

今天主要的开发内容就是Store Buffer（SB）的实现。相较于上一版Ziron，这次的SB实现主要有以下几个主要改进：

1. 添加了rptr（ready pointer），用于指示SB中已经验证为可提交的存储请求；添加了cptr（commit pointer），用于指示SB中已经完成提交写入的存储请求，这个指针的更新信息由DCache提供。有了这两个指针，就可以通过二者差值获取到SB是否将所有已被验证的存储请求都提交了。

2. 在1的基础上，现在计划ROB提交特权指令时会等待所有SB中的存储请求都完成提交，再执行一次流水线flush。这次flush会清空SB所有表项，用来避免使用虚拟地址索引SB的冲突情况。

3. load访问SB时，这里不再基于tail指针重排SB表项，因为这样会导致电路面积的大量浪费（新表中每一个表项都是一个Mux1H的开销）。通过实现左右循环移位函数，我将比较结果先基于tail指针右移，通过独热优先编码器编码后，再左移，之后对SB中的命中表项进行选择。


## 2025-02-03

1. 将uncache write停留在c1s3的停止条件修改为：c2s3中该指令在SB中的位置出现。为此，SB将会把写入的表项号向后传递一级
2. 思考了一个问题：到底应该把SB放在DCache FSM级，还是DCache Data级。
    - 对于前者，可以直接使用物理地址进行索引，且store提交时也可以使用物理地址，但全相连查找的时序加上时序，加上FSM对输出数据信号的多选，加上输出信号的拼接，很可能成为一个很大的问题。
    - 对于后者，只能使用虚拟地址进行索引，且在FSM级获得到物理地址后，还需要写入到对应的SB表项中，而且由于查找时序，FSM级还需要一个额外的缓存项，假设store后面紧跟一条load，就需要利用此缓存项来判断load是否需要读取最新的写入数据。与此同时，SB每一项的存储成本也会加大，而且进程切换还需要等待所有写入都完成。
    - 综合考虑，将SB放在DCache FSM级。一个备用的解决方案是，将对写回数据是从Cache还是SB中读取的判断延迟到WB级——这并不会导致前递逻辑过于冗长。
3. 对于load对load和store的唤醒，只有当一条load到达DCache Data段时，才可以对后续的load和store执行唤醒。由于单流水线阻塞，这个唤醒不需要推测。为了功能完整性，我决定将DCache拓展一级，这一级专门用于对读出数据进行选择，选择后的数据会直接接入前递和写回逻辑中。
    - 注意，这一级和前一级的段间寄存器在miss时需要冲刷。

## 2025-02-04

今天构思了DCache和L2Cache的接口，并实现了DCache的主状态机。

1. DCache的结构与之前相同，两条通道，一条用于读和写第一次进入，另一条用于写提交。
2. 提交通道暂定为不使用状态机，使得DCache采用了写直达、写不分配的策略。读写通道只会向L2请求并读取数据，不会写入任何数据。
3. 采用写不分配的策略的主要原因有：简化DCache状态机设计、减少写提交通道的写入阻塞（只需要将写请求提交出去就好，流水线不再关心是否写完，这个写完确认只会影响SB的的cptr指针）。
4. 为了保证L2中强序非缓存读写的顺序，依然保持了原有的ICache和DCache各使用一个通道的设计（如果单独使用一个通道负责DCache写穿透，那么其实会导致读通道阻塞时写通道难以控制基本顺序）。特别需要注意的是，DCache两个通道不能同时发起对L2的访问，也就是说**L2的DCache通道同时只需要处理写或读即可**。

## 2025-02-05

今天实现了DCache的第一个通道，对一些问题又进行了改进

1. uncache write处理：之前的方案里，如果把请求停在c1s3，那么这个指令也不会通知ROB，导致ROB不会正常提交，SB中该请求也不会被提交，因此c2s3不会出现这个SB的idx。所以我将停止条件修改为：SB中所有表项全部提交完成。

2. 为了适应1的修改，SB的clear信号改为cptr和tptr来确定。由于SB在s3，当miss时不会写入，所以一定能等到所有的写请求全部提交完毕。

3. L2的uncache访问在生成读取数据时有问题，已做修改。

## 2025-02-06

今天主要是完成了DCache代码的编写，并解决了几个潜在的问题：

1. 当DCache接收到flush信号时，DCache各个阶段的寄存器都会被清空。如果此时正处于缺失处理，那么会等待完成本次处理（为了适应AXI总线访存的不可中断）；若正处于uncache的等待状态，则直接让状态机回归IDLE并清空miss信号。

## 2025-02-08

今天完成了DCache和L2Cache系统的整体测试，已经基本能够确认通过随机访问测试。

1. L2Cache中，dcache的hazard现在修改为，只要c2s3中存在写请求，那么dcache的hazard就为真。

2. DCacheFSM的重要修改：对L2的访问只能在L2可接受时持续一个周期，因此在外部增加了取边沿。

3. 所有DCache读通道的缺失请求必须等待SB中所有写请求完成，才能继续。这样做有两个原因，其一，避免了写提交通道在读缺失时向被替换后的行写入数据，使得恰好在命中时将数据写入了错误的行；其二，避免了读L2时写同一行导致换回的数据不及时。

4. 将SB满连接入阻塞体系，相应的修改了各个段间寄存器的执行逻辑

5. SB的指针和full信号生成做了重大修改：现在full不会再由hptr决定，而是由cptr和tptr决定。同时，之前SB查找逻辑的优先级有问题，已作出修改。

6. 修改了SB读命中逻辑和指针增长方向，用来适应PriorityEncoderOH的输出方式。

# 2025-02-10

1. 更新了DCache在load乱序不可缓存发射时miss寄存器的更新情况，并精简了状态机。

2. 提升了DCache在随机访存过程中的性能，现在非uncache的访问不会等待SB全部提交，而是在处理前锁定SB，在等待L2Cache返回时，DCache中所有残留的写请求必定都会提交进入L2Cache。

3. 进一步丰富了返回缓存rbuf的功能，现在当一次缺失处理开始时，rbuf会记录那些与第一通道缺失的读请求在同一行中的写请求，并将其写入数据及时写入rbuf。L2返回数据后，这些被写入的位置不会被覆盖，从而实现了数据及时重填入L1Cache。

4. 借助Claude修改了测试生成代码，使用了共享Random对象和Array进行改进，大幅度提升了生成的性能。

# 2025-02-16

1. 修复了SB的重要Bug：当SB被lock的时候，eptyn应当保持不变，否则可能会导致满时判空。

2. ICache重要更新：为了保证每个周期都能给出nfetch条指令，ICache现在每一行都会额外存储nfetch-1条指令。为了能够跨行获取到这些数据，ICache每次缺失都会向L2Cache连续请求两次，请求地址为连续的两次行地址。在较大概率下，这样只会引发多一个周期的等待。而对于L2Cache也需要跨行的情况，这样也不失为一种合理的预取。

3. Cache测试环境中，修复了AXI总线写入数据的问题，也将每个Memory Item中的Data改为Long型。

# 2025-02-17
今天主要是L2Cache的更新：

1. 在ICache和DCache同时都需要访问的地址范围内，我剥夺了ICache通道向主存写入数据的权利。现在，如果ICache先取得了这一块数据，当DCache再访问这一块数据时，会无效掉ICache请求的这一块，并将数据重新取入DCache所管辖的存储器。这样一来，ICache管辖的存储器内就不会有脏数据，同时也避免了**ICache需要写回一个块的过程中DCache再次写入这个块，导致ICache重填时覆盖掉写入结果**的情况。

2. 在以上改动的基础上，由于ICache不会写回，所以AXI-Arbiter的写状态机减少了一半的状态。同时由于ICache数据一定不脏，脏位表存储也减少了一半。

3. 现在的L2Cache状态机，每个通道状态机只能控制自己存储器的写使能，而不能控制其他存储器。但DCache通道的状态机可以将ICache通道的vld表项无效化。
