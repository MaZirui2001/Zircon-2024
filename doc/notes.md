# Zircon2024开发心得与思考

## 2025-01-31

今天对DCache架构进行了思考。主要的成果如下：

1. True Dual BRAM实现了store提交的“超车”

2. DCache的违例情况：

    - uncache store：需要保证之前的所有读写都完成，且之后的所有读操作都不能执行；
        - 所有uncache写会停留在c1s3中，直至**c2s3中该指令的rob_idx出现**。
    - uncache load：需要保证之前的所有读写都完成
        - 保证之前的读的顺序：乱序发射出的load会被强制送回发射队列重新执行
        - 保证之前的写的顺序：所有uncache读会停留在c1s3中，直至**该指令成为ROB中最后一条指令，且Store Buffer所有待提交指令全部完成执行**。

## 2025-02-01

今天主要的开发内容就是Store Buffer（SB）的实现。相较于上一版Ziron，这次的SB实现主要有以下几个主要改进：

1. 添加了rptr（ready pointer），用于指示SB中已经验证为可提交的存储请求；添加了cptr（commit pointer），用于指示SB中已经完成提交写入的存储请求，这个指针的更新信息由DCache提供。有了这两个指针，就可以通过二者差值获取到SB是否将所有已被验证的存储请求都提交了。

2. 在1的基础上，现在计划ROB提交特权指令时会等待所有SB中的存储请求都完成提交，再执行一次流水线flush。这次flush会清空SB所有表项，用来避免使用虚拟地址索引SB的冲突情况。

3. load访问SB时，这里不再基于tail指针重排SB表项，因为这样会导致电路面积的大量浪费（新表中每一个表项都是一个Mux1H的开销）。通过实现左右循环移位函数，我将比较结果先基于tail指针右移，通过独热优先编码器编码后，再左移，之后对SB中的命中表项进行选择。


## 2025-02-03

1. 将uncache write停留在c1s3的停止条件修改为：c2s3中该指令在SB中的位置出现。为此，SB将会把写入的表项号向后传递一级
2. 思考了一个问题：到底应该把SB放在DCache FSM级，还是DCache Data级。
    - 对于前者，可以直接使用物理地址进行索引，且store提交时也可以使用物理地址，但全相连查找的时序加上时序，加上FSM对输出数据信号的多选，加上输出信号的拼接，很可能成为一个很大的问题。
    - 对于后者，只能使用虚拟地址进行索引，且在FSM级获得到物理地址后，还需要写入到对应的SB表项中，而且由于查找时序，FSM级还需要一个额外的缓存项，假设store后面紧跟一条load，就需要利用此缓存项来判断load是否需要读取最新的写入数据。与此同时，SB每一项的存储成本也会加大，而且进程切换还需要等待所有写入都完成。
    - 综合考虑，将SB放在DCache FSM级。一个备用的解决方案是，将对写回数据是从Cache还是SB中读取的判断延迟到WB级——这并不会导致前递逻辑过于冗长。
3. 对于load对load和store的唤醒，只有当一条load到达DCache Data段时，才可以对后续的load和store执行唤醒。由于单流水线阻塞，这个唤醒不需要推测。为了功能完整性，我决定将DCache拓展一级，这一级专门用于对读出数据进行选择，选择后的数据会直接接入前递和写回逻辑中。
    - 注意，这一级和前一级的段间寄存器在miss时需要冲刷。

## 2025-02-04

今天构思了DCache和L2Cache的接口，并实现了DCache的主状态机。

1. DCache的结构与之前相同，两条通道，一条用于读和写第一次进入，另一条用于写提交。
2. 提交通道暂定为不使用状态机，使得DCache采用了写直达、写不分配的策略。读写通道只会向L2请求并读取数据，不会写入任何数据。
3. 采用写不分配的策略的主要原因有：简化DCache状态机设计、减少写提交通道的写入阻塞（只需要将写请求提交出去就好，流水线不再关心是否写完，这个写完确认只会影响SB的的cptr指针）。
4. 为了保证L2中强序非缓存读写的顺序，依然保持了原有的ICache和DCache各使用一个通道的设计（如果单独使用一个通道负责DCache写穿透，那么其实会导致读通道阻塞时写通道难以控制基本顺序）。特别需要注意的是，DCache两个通道不能同时发起对L2的访问，也就是说**L2的DCache通道同时只需要处理写或读即可**。

## 2025-02-05

今天实现了DCache的第一个通道，对一些问题又进行了改进

1. uncache write处理：之前的方案里，如果把请求停在c1s3，那么这个指令也不会通知ROB，导致ROB不会正常提交，SB中该请求也不会被提交，因此c2s3不会出现这个SB的idx。所以我将停止条件修改为：SB中所有表项全部提交完成。

2. 为了适应1的修改，SB的clear信号改为cptr和tptr来确定。由于SB在s3，当miss时不会写入，所以一定能等到所有的写请求全部提交完毕。

3. L2的uncache访问在生成读取数据时有问题，已做修改。

## 2025-02-06

今天主要是完成了DCache代码的编写，并解决了几个潜在的问题：

1. 当DCache接收到flush信号时，DCache各个阶段的寄存器都会被清空。如果此时正处于缺失处理，那么会等待完成本次处理（为了适应AXI总线访存的不可中断）；若正处于uncache的等待状态，则直接让状态机回归IDLE并清空miss信号。

## 2025-02-08

今天完成了DCache和L2Cache系统的整体测试，已经基本能够确认通过随机访问测试。

1. L2Cache中，dcache的hazard现在修改为，只要c2s3中存在写请求，那么dcache的hazard就为真。

2. DCacheFSM的重要修改：对L2的访问只能在L2可接受时持续一个周期，因此在外部增加了取边沿。

3. 所有DCache读通道的缺失请求必须等待SB中所有写请求完成，才能继续。这样做有两个原因，其一，避免了写提交通道在读缺失时向被替换后的行写入数据，使得恰好在命中时将数据写入了错误的行；其二，避免了读L2时写同一行导致换回的数据不及时。

4. 将SB满连接入阻塞体系，相应的修改了各个段间寄存器的执行逻辑

5. SB的指针和full信号生成做了重大修改：现在full不会再由hptr决定，而是由cptr和tptr决定。同时，之前SB查找逻辑的优先级有问题，已作出修改。

6. 修改了SB读命中逻辑和指针增长方向，用来适应PriorityEncoderOH的输出方式。

# 2025-02-10

1. 更新了DCache在load乱序不可缓存发射时miss寄存器的更新情况，并精简了状态机。

2. 提升了DCache在随机访存过程中的性能，现在非uncache的访问不会等待SB全部提交，而是在处理前锁定SB，在等待L2Cache返回时，DCache中所有残留的写请求必定都会提交进入L2Cache。

3. 进一步丰富了返回缓存rbuf的功能，现在当一次缺失处理开始时，rbuf会记录那些与第一通道缺失的读请求在同一行中的写请求，并将其写入数据及时写入rbuf。L2返回数据后，这些被写入的位置不会被覆盖，从而实现了数据及时重填入L1Cache。

4. 借助Claude修改了测试生成代码，使用了共享Random对象和Array进行改进，大幅度提升了生成的性能。

# 2025-02-16

1. 修复了SB的重要Bug：当SB被lock的时候，eptyn应当保持不变，否则可能会导致满时判空。

2. ICache重要更新：为了保证每个周期都能给出nfetch条指令，ICache现在每一行都会额外存储nfetch-1条指令。为了能够跨行获取到这些数据，ICache每次缺失都会向L2Cache连续请求两次，请求地址为连续的两次行地址。在较大概率下，这样只会引发多一个周期的等待。而对于L2Cache也需要跨行的情况，这样也不失为一种合理的预取。

3. Cache测试环境中，修复了AXI总线写入数据的问题，也将每个Memory Item中的Data改为Long型。

# 2025-02-17
今天主要是L2Cache的更新：

1. 在ICache和DCache同时都需要访问的地址范围内，我剥夺了ICache通道向主存写入数据的权利。现在，如果ICache先取得了这一块数据，当DCache再访问这一块数据时，会无效掉ICache请求的这一块，并将数据重新取入DCache所管辖的存储器。这样一来，ICache管辖的存储器内就不会有脏数据，同时也避免了**ICache需要写回一个块的过程中DCache再次写入这个块，导致ICache重填时覆盖掉写入结果**的情况。

2. 在以上改动的基础上，由于ICache不会写回，所以AXI-Arbiter的写状态机减少了一半的状态。同时由于ICache数据一定不脏，脏位表存储也减少了一半。

3. 现在的L2Cache状态机，每个通道状态机只能控制自己存储器的写使能，而不能控制其他存储器。但DCache通道的状态机可以将ICache通道的vld表项无效化。

# 2025-02-18

今天主要思考了译码逻辑的问题。

1. 为了进一步缩短前端流水级，我计划在ICache最后一级数据出来后直接使用尽可能精简的逻辑来译码出rj、rk、rd和各自的valid信号。根据我的观察，ListLookup并不可靠，因为里面有大量的迭代运算——这会导致时序爆炸。因此，我直接观察指令集各个指令的编码，用我能想到的最简方法来获取到各个值。同时，我还会保证，如果rx是0，那么rx_vld一定是0。同样地，如果rx_vld是0，那么rx一定是0。

2. 在译码逻辑中，我主要设计了op和imm的生成逻辑。不同于以往设计，本次我将br_type和mem_type也都复用到了op中。对于不同指令，其各个位的意义不同。
    - alu指令：\[5\]位是src1来源，\[4\]位是src2来源，\[3:0\]位是alu操作。
    - br指令：\[4:0\]位是跳转类型。
    - mem指令： \[5\]位表示存储，\[4\]位表示加载，\[3\]位表示是否是原子操作，\[2\]位表示零拓展，\[1:0\]位表示访存长度。

# 2025-02-25

今天设计了ROB和物理寄存器空闲列表，并修改了重命名相关的内容。

1. ROB复用了Cluster Index FIFO的结构，但是为了减少写入的组合逻辑，我重新修改了Index FIFO的写入逻辑，当类型是ROB表项时，入队逻辑只会写入前端的数据域，写入逻辑只会写入后端的数据域。

2. 物理寄存器空闲列表依然复用了Cluster Index FIFO的结构，不同的是，入队和出队的请求并不是压缩的（也就是说，可能入队端口1号和3号请求入队，但2号没有请求入队）。所以在分配物理寄存器时，我使用独热码创建了一个映射表，通过转置来完成双射，从而实现了复用。同时，我也检查出了之前在发射队列的空闲列表中存在的入队逻辑问题，并总结出了一个规律：非压缩一端到队列的映射是容易确定的，而队列出入端口到非压缩一端的映射只能通过矩阵转置来实现。

3. 重命名阶段，我重新修改了SRat：既然我们只会在提交阶段维护一个完全正确的映射表，我们为什么还要大费周章的复用端口来复原数据呢？我们完全可以一个周期把ARat中所有数据都福源道SRat中——这只是简单的寄存器复制而已。

# 2025-03-15

近期完成了SRT2除法器的设计。除去从各大网站上学习的内容之外，我还发现了一些其他问题：

1. SRT2除法中，一共需要除数前导零数减去被除数前导零数个周期，但若被除数前导零数大于除数前导零数，则直接结束。
2. SRT2除法需要将先除数左移直至最高位是1为止，被除数需要左移到次高位是1为止（具体实现应该是，被除数先左移到最高位是1，然后右移一位）。但是如果被除数前导零个数大于除数前导零个数，则需要将被除数左移除数前导零个数，这是因为这种情况余数就等于被除数，此时恰好能把结果设为被除数本身。
3. 还需要思考一种特殊情况：被除数和除数的前导零个数相同。这是有两种情况：被除数大于等于除数时，仍然需要进行一步运算（减法），才能获得余数。被除数小于除数时，余数就等于被除数。为了减少硬件复杂度，我将这两种情况合并，而第2点的处理方法恰好能处理这种问题（当被除数和除数前导零个数相同时，被除数左移后仍需要一个周期做减法，但此时余数是负数，后续电路会将这个余数加回来）。

# 2025-03-30

今天搭建完了Difftest环境，开始了测试。不过测试遇到了很多问题：

1. 使用next pc来查找ICache会遇到一个问题：如果ICache miss时，flush信号来了，那么PC会直接被冲刷，导致next pc会变成这个冲刷的PC的next pc，导致丢掉一组指令。我采用的解决方案是，冲刷时也会同步更新掉ICache第一、二流水级间的段间寄存器，同时如果此时恰好处于wait状态，那么addr_1H也需要选择第一级流水，保证地址的正确性。

2. 唤醒逻辑有误：我们需要在Rename阶段判断组内的RAW问题，并结合后续的Ready Board来判断是否入队就唤醒。不能只依靠唤醒总线来判断（一个好玩且搞笑的事情是，一看波型竟然发现Ready Board的ready信号没了，那Ready Board还有什么用呢？）

# 2025-04-01

今天成功通过了add测试，一定程度上验证了计算单元的正确性。下面记录一下Bug的由来和修改方式：

1. op位问题：只通过op\[6\]来判断是否为store并不准确，因为这一位在op当做ALU操作码时记录了src2的来源。因此，在写入ROB的时候，还需要通过func来判断。

2. 流水线停顿信号问题：Rename空闲列表为空时，应该阻止这一级指令向后流动；ROB满时，应该阻止发射队列写入新指令。

3. PC提交重定向问题：相比较之前的PC更新策略，新设计更注重加法器的复用，特别是针对在提交段不跳转导致PC+4的指令，Commit段从前会用加法器加好送到PC，现在直接复用PC顺序+4的加法器，通过修改其源操作数来完成重定向。

4. ICache问题：Commit段flush时，ICache的rreq同样应该被置为有效，因为NPC此时已经产生，故需要立即重新取指；同时，如果ICache处于miss状态且flush信号有效，由于ICache不可中断，故必须强行更新s1和s2段间寄存器。这里还有一些细节问题：
   - FSM的wait状态，这里会重新定向bram的读取地址，但如果此时flush有效，那么下一个周期读取的数据应该从s1读取，而不是常规情况下的s2。
   - FSM的idke状态，如果stall时flush有效，那么应该读取s1阶段的地址，而不是常规情况下的s2。

5. PReg\_Free\_List冲刷问题：在之前的设计中，重命名映射表所有项均为0，所以初期的提交需要屏蔽掉那些pprd为0的指令。但这样的话，我们就需要额外的一个指针用来恢复head指针（因为tail指针在初期遇到pprd为0的指令不会自增，而完全head只要提交了一条rd有效的指令就应该自增，一旦使用tail恢复，那么就会出现重新分配一些已经提交的物理寄存器的情况）。所以我采用了一种新的方法：为重命名表赋予初值（第i个寄存器重命名到物理寄存器i），且让空闲列表的表项中不含这些初值。这样既能节省了空闲列表的表项（现在仅需npreg-32个），还能够使用tail来恢复head指针，而且不再需要再提交时判断pprd是否为0（因为一定不会为0）。

